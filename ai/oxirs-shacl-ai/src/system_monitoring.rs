//! Comprehensive System Monitoring for SHACL-AI
//!
//! This module provides real-time monitoring capabilities for:
//! - Performance metrics and trends
//! - Quality assessment and degradation detection
//! - Error tracking and analysis
//! - Resource utilization monitoring
//! - Health checks and alerting

use chrono::{DateTime, Utc};
use scirs2_core::random::{Rng, Random};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::{Arc, Mutex, RwLock};
use std::time::{Duration, Instant};
use uuid::Uuid;

use crate::{
    forecasting_models::{TimeSeries, TimeSeriesDataPoint},
    Result, ShaclAiError,
};

/// Main system monitoring engine
#[derive(Debug)]
pub struct SystemMonitor {
    config: MonitoringConfig,
    metrics_collector: Arc<Mutex<MetricsCollector>>,
    alert_manager: Arc<Mutex<AlertManager>>,
    dashboard: Arc<RwLock<MonitoringDashboard>>,
    health_checker: Arc<Mutex<HealthChecker>>,
    storage: Arc<Mutex<MonitoringStorage>>,
    notifier: Arc<Mutex<NotificationEngine>>,
}

/// Configuration for system monitoring
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MonitoringConfig {
    /// Enable real-time monitoring
    pub enable_real_time: bool,

    /// Metrics collection interval in seconds
    pub collection_interval_secs: u64,

    /// Maximum history retention days
    pub retention_days: u32,

    /// Enable alerting
    pub enable_alerting: bool,

    /// Health check interval in seconds
    pub health_check_interval_secs: u64,

    /// Enable performance tracking
    pub enable_performance_tracking: bool,

    /// Enable quality tracking
    pub enable_quality_tracking: bool,

    /// Enable error tracking
    pub enable_error_tracking: bool,

    /// Dashboard refresh interval in seconds
    pub dashboard_refresh_secs: u64,

    /// Maximum metrics buffer size
    pub max_buffer_size: usize,

    /// Enable predictive alerting
    pub enable_predictive_alerts: bool,

    /// Alert thresholds
    pub alert_thresholds: AlertThresholds,

    /// Notification channels
    pub notification_channels: Vec<NotificationChannel>,
}

impl Default for MonitoringConfig {
    fn default() -> Self {
        Self {
            enable_real_time: true,
            collection_interval_secs: 60,
            retention_days: 30,
            enable_alerting: true,
            health_check_interval_secs: 30,
            enable_performance_tracking: true,
            enable_quality_tracking: true,
            enable_error_tracking: true,
            dashboard_refresh_secs: 30,
            max_buffer_size: 10000,
            enable_predictive_alerts: true,
            alert_thresholds: AlertThresholds::default(),
            notification_channels: vec![NotificationChannel::Console],
        }
    }
}

/// Alert threshold configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertThresholds {
    /// Performance thresholds
    pub max_response_time_ms: f64,
    pub min_throughput: f64,
    pub max_error_rate: f64,
    pub max_memory_usage_percent: f64,
    pub max_cpu_usage_percent: f64,

    /// Quality thresholds
    pub min_quality_score: f64,
    pub max_quality_degradation_percent: f64,

    /// System health thresholds
    pub max_consecutive_failures: u32,
    pub min_uptime_percent: f64,

    /// Prediction thresholds
    pub prediction_confidence_threshold: f64,
    pub trend_significance_threshold: f64,
}

impl Default for AlertThresholds {
    fn default() -> Self {
        Self {
            max_response_time_ms: 5000.0,
            min_throughput: 100.0,
            max_error_rate: 0.05,
            max_memory_usage_percent: 85.0,
            max_cpu_usage_percent: 80.0,
            min_quality_score: 0.8,
            max_quality_degradation_percent: 10.0,
            max_consecutive_failures: 5,
            min_uptime_percent: 99.0,
            prediction_confidence_threshold: 0.7,
            trend_significance_threshold: 0.8,
        }
    }
}

/// Notification channels for alerts
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum NotificationChannel {
    Console,
    Email(String),
    Slack(String),
    Webhook(String),
    SMS(String),
    PagerDuty(String),
}

/// Metrics collection engine
#[derive(Debug)]
pub struct MetricsCollector {
    performance_metrics: VecDeque<PerformanceMetric>,
    quality_metrics: VecDeque<QualityMetric>,
    error_metrics: VecDeque<ErrorMetric>,
    system_metrics: VecDeque<SystemMetric>,
    custom_metrics: HashMap<String, VecDeque<CustomMetric>>,
    collection_start_time: Instant,
    last_collection_time: Option<Instant>,
}

/// Performance metric data point
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMetric {
    pub timestamp: DateTime<Utc>,
    pub response_time_ms: f64,
    pub throughput: f64,
    pub concurrent_requests: u32,
    pub memory_usage_mb: f64,
    pub cpu_usage_percent: f64,
    pub disk_io_mb_s: f64,
    pub network_io_mb_s: f64,
    pub gc_time_ms: Option<f64>,
    pub cache_hit_rate: Option<f64>,
    pub tags: HashMap<String, String>,
}

/// Quality metric data point
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QualityMetric {
    pub timestamp: DateTime<Utc>,
    pub overall_quality_score: f64,
    pub completeness_score: f64,
    pub consistency_score: f64,
    pub accuracy_score: f64,
    pub validity_score: f64,
    pub timeliness_score: f64,
    pub data_volume: u64,
    pub shapes_validated: u32,
    pub constraints_checked: u32,
    pub quality_trend: QualityTrend,
    pub degradation_indicators: Vec<String>,
}

/// Quality trend indicators
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum QualityTrend {
    Improving,
    Stable,
    Degrading,
    Critical,
    Unknown,
}

/// Error metric data point
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ErrorMetric {
    pub timestamp: DateTime<Utc>,
    pub error_type: ErrorType,
    pub error_count: u32,
    pub error_rate: f64,
    pub severity: ErrorSeverity,
    pub source_component: String,
    pub error_message: String,
    pub stack_trace: Option<String>,
    pub resolution_time_ms: Option<f64>,
    pub impact_scope: ErrorImpactScope,
    pub recovery_action: Option<String>,
}

/// Types of errors tracked
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ErrorType {
    ValidationError,
    PerformanceTimeout,
    SystemFailure,
    DataQualityIssue,
    ConfigurationError,
    SecurityViolation,
    ResourceExhaustion,
    NetworkError,
    DatabaseError,
    Unknown,
}

/// Error severity levels
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub enum ErrorSeverity {
    Info,
    Warning,
    Error,
    Critical,
    Fatal,
}

/// Error impact scope
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ErrorImpactScope {
    Single,
    Component,
    System,
    Cluster,
    Global,
}

/// System resource metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SystemMetric {
    pub timestamp: DateTime<Utc>,
    pub cpu_cores: u32,
    pub total_memory_mb: f64,
    pub available_memory_mb: f64,
    pub disk_total_gb: f64,
    pub disk_available_gb: f64,
    pub network_connections: u32,
    pub uptime_seconds: u64,
    pub load_average: Vec<f64>,
    pub swap_usage_mb: f64,
    pub process_count: u32,
    pub thread_count: u32,
}

/// Custom metric for application-specific tracking
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CustomMetric {
    pub timestamp: DateTime<Utc>,
    pub name: String,
    pub value: f64,
    pub unit: String,
    pub labels: HashMap<String, String>,
    pub description: String,
}

/// Alert management system
#[derive(Debug)]
pub struct AlertManager {
    active_alerts: HashMap<String, Alert>,
    alert_history: VecDeque<AlertHistoryEntry>,
    alert_rules: Vec<AlertRule>,
    notification_queue: VecDeque<AlertNotification>,
    suppression_rules: Vec<SuppressionRule>,
}

/// Alert definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Alert {
    pub id: String,
    pub alert_type: AlertType,
    pub severity: AlertSeverity,
    pub title: String,
    pub description: String,
    pub triggered_at: DateTime<Utc>,
    pub metric_value: f64,
    pub threshold_value: f64,
    pub source_metric: String,
    pub related_components: Vec<String>,
    pub suggested_actions: Vec<String>,
    pub auto_resolve: bool,
    pub escalation_level: u32,
    pub tags: HashMap<String, String>,
}

/// Types of alerts
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AlertType {
    Performance,
    Quality,
    Error,
    System,
    Security,
    Predictive,
    Custom,
}

/// Alert severity levels
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub enum AlertSeverity {
    Info,
    Warning,
    Critical,
    Emergency,
}

/// Alert rule configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertRule {
    pub id: String,
    pub name: String,
    pub condition: AlertCondition,
    pub threshold: f64,
    pub duration_secs: u64,
    pub severity: AlertSeverity,
    pub enabled: bool,
    pub notification_channels: Vec<NotificationChannel>,
    pub auto_resolve: bool,
    pub escalation_rules: Vec<EscalationRule>,
}

/// Alert condition types
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AlertCondition {
    GreaterThan,
    LessThan,
    Equals,
    NotEquals,
    PercentageChange,
    Trend,
    Anomaly,
    Threshold,
}

/// Escalation rule for alerts
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EscalationRule {
    pub delay_secs: u64,
    pub severity: AlertSeverity,
    pub notification_channels: Vec<NotificationChannel>,
    pub actions: Vec<String>,
}

/// Alert history entry
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertHistoryEntry {
    pub alert: Alert,
    pub resolved_at: Option<DateTime<Utc>>,
    pub resolution_method: Option<ResolutionMethod>,
    pub notifications_sent: u32,
    pub escalations: u32,
    pub mean_time_to_resolution: Option<Duration>,
}

/// How an alert was resolved
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ResolutionMethod {
    AutoResolved,
    ManualResolved,
    SuppressedResolved,
    TimeoutResolved,
}

/// Alert notification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertNotification {
    pub id: String,
    pub alert_id: String,
    pub channel: NotificationChannel,
    pub message: String,
    pub timestamp: DateTime<Utc>,
    pub status: NotificationStatus,
    pub retry_count: u32,
    pub delivery_time: Option<DateTime<Utc>>,
}

/// Notification delivery status
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum NotificationStatus {
    Pending,
    Sent,
    Delivered,
    Failed,
    Retrying,
}

/// Alert suppression rule
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SuppressionRule {
    pub id: String,
    pub name: String,
    pub condition: SuppressionCondition,
    pub duration_secs: u64,
    pub enabled: bool,
}

/// Suppression conditions
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SuppressionCondition {
    MaintenanceWindow,
    ComponentMaintenance,
    AlertFlood,
    DependencyDown,
    Custom(String),
}

/// Real-time monitoring dashboard
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MonitoringDashboard {
    pub last_updated: DateTime<Utc>,
    pub overall_health: SystemHealth,
    pub performance_summary: PerformanceSummary,
    pub quality_summary: QualitySummary,
    pub error_summary: ErrorSummary,
    pub active_alerts: Vec<Alert>,
    pub trends: TrendAnalysis,
    pub recommendations: Vec<MonitoringRecommendation>,
    pub uptime_stats: UptimeStatistics,
    pub resource_utilization: ResourceUtilization,
}

/// Overall system health status
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub enum SystemHealth {
    Healthy,
    Warning,
    Critical,
    Degraded,
    Unknown,
}

/// Performance summary for dashboard
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceSummary {
    pub avg_response_time_ms: f64,
    pub current_throughput: f64,
    pub peak_throughput_24h: f64,
    pub error_rate_percent: f64,
    pub availability_percent: f64,
    pub response_time_trend: TrendDirection,
    pub throughput_trend: TrendDirection,
    pub performance_score: f64,
}

/// Quality summary for dashboard
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QualitySummary {
    pub overall_quality_score: f64,
    pub quality_trend: QualityTrend,
    pub issues_detected: u32,
    pub shapes_validated_24h: u32,
    pub quality_improvement_percent: f64,
    pub data_completeness_percent: f64,
    pub consistency_score: f64,
}

/// Error summary for dashboard
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ErrorSummary {
    pub total_errors_24h: u32,
    pub critical_errors_24h: u32,
    pub error_rate_trend: TrendDirection,
    pub mean_time_to_resolution_min: f64,
    pub most_frequent_error_type: ErrorType,
    pub resolved_errors_24h: u32,
    pub unresolved_errors: u32,
}

/// Trend direction indicator
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TrendDirection {
    Improving,
    Stable,
    Degrading,
    Volatile,
    Unknown,
}

/// Trend analysis for various metrics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrendAnalysis {
    pub performance_trends: HashMap<String, TrendData>,
    pub quality_trends: HashMap<String, TrendData>,
    pub error_trends: HashMap<String, TrendData>,
    pub usage_trends: HashMap<String, TrendData>,
    pub seasonal_patterns: Vec<SeasonalPattern>,
    pub anomalies_detected: Vec<AnomalyDetection>,
}

/// Trend data for a specific metric
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrendData {
    pub direction: TrendDirection,
    pub change_percent: f64,
    pub confidence: f64,
    pub prediction_7d: f64,
    pub prediction_30d: f64,
    pub significance: f64,
    pub correlation_factors: Vec<String>,
}

/// Seasonal pattern detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SeasonalPattern {
    pub metric_name: String,
    pub pattern_type: PatternType,
    pub period_hours: f64,
    pub amplitude: f64,
    pub confidence: f64,
    pub next_peak: DateTime<Utc>,
    pub next_trough: DateTime<Utc>,
}

/// Types of patterns detected
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PatternType {
    Daily,
    Weekly,
    Monthly,
    Custom(f64),
}

/// Anomaly detection result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyDetection {
    pub metric_name: String,
    pub detected_at: DateTime<Utc>,
    pub anomaly_type: AnomalyType,
    pub severity: f64,
    pub expected_value: f64,
    pub actual_value: f64,
    pub confidence: f64,
    pub context: HashMap<String, String>,
}

/// Monitoring recommendations
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MonitoringRecommendation {
    pub id: String,
    pub title: String,
    pub description: String,
    pub category: RecommendationCategory,
    pub priority: u8,
    pub impact: String,
    pub actions: Vec<String>,
    pub estimated_effort: String,
    pub created_at: DateTime<Utc>,
}

/// Recommendation categories
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RecommendationCategory {
    Performance,
    Reliability,
    Security,
    Cost,
    Maintenance,
    Scaling,
}

/// Uptime statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UptimeStatistics {
    pub current_uptime_secs: u64,
    pub uptime_percent_24h: f64,
    pub uptime_percent_7d: f64,
    pub uptime_percent_30d: f64,
    pub longest_uptime_secs: u64,
    pub total_downtime_24h_secs: u64,
    pub outage_count_24h: u32,
    pub mean_time_between_failures_hours: f64,
}

/// Resource utilization summary
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResourceUtilization {
    pub cpu_usage_percent: f64,
    pub memory_usage_percent: f64,
    pub disk_usage_percent: f64,
    pub network_usage_mb_s: f64,
    pub connection_count: u32,
    pub thread_count: u32,
    pub file_descriptor_count: u32,
    pub resource_trends: HashMap<String, TrendDirection>,
}

/// Health checker for system components
#[derive(Debug)]
pub struct HealthChecker {
    component_status: HashMap<String, ComponentHealth>,
    health_checks: Vec<HealthCheck>,
    last_check_time: Option<Instant>,
    check_history: VecDeque<HealthCheckResult>,
}

/// Component health status
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComponentHealth {
    pub name: String,
    pub status: HealthStatus,
    pub last_check: DateTime<Utc>,
    pub response_time_ms: f64,
    pub uptime_percent: f64,
    pub error_count: u32,
    pub dependencies: Vec<String>,
    pub health_score: f64,
}

/// Health status levels
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub enum HealthStatus {
    Healthy,
    Warning,
    Critical,
    Unknown,
    Maintenance,
}

/// Health check definition
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheck {
    pub name: String,
    pub component: String,
    pub check_type: HealthCheckType,
    pub interval_secs: u64,
    pub timeout_secs: u64,
    pub enabled: bool,
    pub critical: bool,
}

/// Types of health checks
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum HealthCheckType {
    Ping,
    HttpEndpoint(String),
    DatabaseConnection,
    FileSystem,
    Memory,
    CPU,
    Custom(String),
}

/// Health check result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HealthCheckResult {
    pub check_name: String,
    pub component: String,
    pub timestamp: DateTime<Utc>,
    pub status: HealthStatus,
    pub response_time_ms: f64,
    pub message: String,
    pub error: Option<String>,
}

/// Monitoring data storage
#[derive(Debug)]
pub struct MonitoringStorage {
    metrics_buffer: HashMap<String, VecDeque<TimeSeriesDataPoint>>,
    aggregated_data: HashMap<String, TimeSeries>,
    retention_policy: RetentionPolicy,
    compression_enabled: bool,
    backup_enabled: bool,
}

/// Data retention policy
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RetentionPolicy {
    pub raw_data_days: u32,
    pub hourly_aggregation_days: u32,
    pub daily_aggregation_days: u32,
    pub monthly_aggregation_days: u32,
    pub compression_threshold_days: u32,
}

/// Notification engine for alerts
#[derive(Debug)]
pub struct NotificationEngine {
    channels: HashMap<String, Box<dyn NotificationSender>>,
    templates: HashMap<String, NotificationTemplate>,
    delivery_status: HashMap<String, NotificationStatus>,
    rate_limits: HashMap<String, RateLimit>,
}

/// Notification template
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NotificationTemplate {
    pub name: String,
    pub subject_template: String,
    pub body_template: String,
    pub format: NotificationFormat,
}

/// Notification formats
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum NotificationFormat {
    Text,
    Html,
    Markdown,
    Json,
}

/// Rate limiting for notifications
#[derive(Debug, Clone)]
pub struct RateLimit {
    pub max_per_hour: u32,
    pub max_per_day: u32,
    pub current_hour_count: u32,
    pub current_day_count: u32,
    pub reset_time: Instant,
}

/// Trait for notification senders
pub trait NotificationSender: Send + Sync + std::fmt::Debug {
    fn send(&self, notification: &AlertNotification) -> Result<()>;
    fn channel_type(&self) -> &str;
    fn is_available(&self) -> bool;
}

impl SystemMonitor {
    /// Create a new system monitor with default configuration
    pub fn new() -> Self {
        Self::with_config(MonitoringConfig::default())
    }

    /// Create a new system monitor with custom configuration
    pub fn with_config(config: MonitoringConfig) -> Self {
        let metrics_collector = Arc::new(Mutex::new(MetricsCollector::new()));
        let alert_manager = Arc::new(Mutex::new(AlertManager::new()));
        let dashboard = Arc::new(RwLock::new(MonitoringDashboard::new()));
        let health_checker = Arc::new(Mutex::new(HealthChecker::new()));
        let storage = Arc::new(Mutex::new(MonitoringStorage::new()));
        let notifier = Arc::new(Mutex::new(NotificationEngine::new()));

        Self {
            config,
            metrics_collector,
            alert_manager,
            dashboard,
            health_checker,
            storage,
            notifier,
        }
    }

    /// Start the monitoring system
    pub fn start(&self) -> Result<()> {
        tracing::info!("Starting comprehensive system monitoring");

        // Initialize monitoring components
        self.initialize_components()?;

        // Start background tasks
        self.start_monitoring_tasks()?;

        tracing::info!("System monitoring started successfully");
        Ok(())
    }

    /// Stop the monitoring system
    pub fn stop(&self) -> Result<()> {
        tracing::info!("Stopping system monitoring");
        // Implementation would stop background tasks
        Ok(())
    }

    /// Record a performance metric
    pub fn record_performance_metric(&self, metric: PerformanceMetric) -> Result<()> {
        self.metrics_collector
            .lock()
            .map_err(|e| {
                ShaclAiError::ShapeManagement(format!("Failed to lock metrics collector: {e}"))
            })?
            .add_performance_metric(metric)?;

        // Trigger real-time analysis if enabled
        if self.config.enable_real_time {
            self.analyze_real_time_metrics()?;
        }

        Ok(())
    }

    /// Record a quality metric
    pub fn record_quality_metric(&self, metric: QualityMetric) -> Result<()> {
        self.metrics_collector
            .lock()
            .map_err(|e| {
                ShaclAiError::ShapeManagement(format!("Failed to lock metrics collector: {e}"))
            })?
            .add_quality_metric(metric)?;

        // Check for quality degradation
        self.check_quality_alerts()?;

        Ok(())
    }

    /// Record an error metric
    pub fn record_error_metric(&self, metric: ErrorMetric) -> Result<()> {
        self.metrics_collector
            .lock()
            .map_err(|e| {
                ShaclAiError::ShapeManagement(format!("Failed to lock metrics collector: {e}"))
            })?
            .add_error_metric(metric)?;

        // Trigger error analysis
        self.analyze_error_patterns()?;

        Ok(())
    }

    /// Get current monitoring dashboard
    pub fn get_dashboard(&self) -> Result<MonitoringDashboard> {
        let dashboard = self
            .dashboard
            .read()
            .map_err(|e| ShaclAiError::ShapeManagement(format!("Failed to read dashboard: {e}")))?;

        Ok(dashboard.clone())
    }

    /// Get active alerts
    pub fn get_active_alerts(&self) -> Result<Vec<Alert>> {
        let alert_manager = self.alert_manager.lock().map_err(|e| {
            ShaclAiError::ShapeManagement(format!("Failed to lock alert manager: {e}"))
        })?;

        Ok(alert_manager.get_active_alerts())
    }

    /// Get system health status
    pub fn get_system_health(&self) -> Result<SystemHealth> {
        let health_checker = self.health_checker.lock().map_err(|e| {
            ShaclAiError::ShapeManagement(format!("Failed to lock health checker: {e}"))
        })?;

        Ok(health_checker.get_overall_health())
    }

    /// Run health checks
    pub fn run_health_checks(&self) -> Result<Vec<HealthCheckResult>> {
        let mut health_checker = self.health_checker.lock().map_err(|e| {
            ShaclAiError::ShapeManagement(format!("Failed to lock health checker: {e}"))
        })?;

        health_checker.run_all_checks()
    }

    /// Add custom metric
    pub fn add_custom_metric(&self, name: String, metric: CustomMetric) -> Result<()> {
        self.metrics_collector
            .lock()
            .map_err(|e| {
                ShaclAiError::ShapeManagement(format!("Failed to lock metrics collector: {e}"))
            })?
            .add_custom_metric(name, metric)?;

        Ok(())
    }

    /// Create custom alert rule
    pub fn create_alert_rule(&self, rule: AlertRule) -> Result<()> {
        self.alert_manager
            .lock()
            .map_err(|e| {
                ShaclAiError::ShapeManagement(format!("Failed to lock alert manager: {e}"))
            })?
            .add_alert_rule(rule)?;

        Ok(())
    }

    /// Get monitoring statistics
    pub fn get_statistics(&self) -> Result<MonitoringStatistics> {
        let metrics_collector = self.metrics_collector.lock().map_err(|e| {
            ShaclAiError::ShapeManagement(format!("Failed to lock metrics collector: {e}"))
        })?;

        let alert_manager = self.alert_manager.lock().map_err(|e| {
            ShaclAiError::ShapeManagement(format!("Failed to lock alert manager: {e}"))
        })?;

        Ok(MonitoringStatistics {
            metrics_collected: metrics_collector.total_metrics_count(),
            alerts_triggered: alert_manager.total_alerts_count(),
            uptime_seconds: metrics_collector.uptime_seconds(),
            data_points_stored: metrics_collector.data_points_count(),
            health_checks_performed: 0, // TODO: implement
            notifications_sent: 0,      // TODO: implement
        })
    }

    /// Initialize monitoring components
    fn initialize_components(&self) -> Result<()> {
        // Initialize health checks
        self.setup_default_health_checks()?;

        // Initialize alert rules
        self.setup_default_alert_rules()?;

        // Initialize notification channels
        self.setup_notification_channels()?;

        Ok(())
    }

    /// Start background monitoring tasks
    fn start_monitoring_tasks(&self) -> Result<()> {
        // In a real implementation, this would spawn background threads
        // for continuous monitoring, data collection, and alerting
        Ok(())
    }

    /// Analyze real-time metrics for immediate alerts
    fn analyze_real_time_metrics(&self) -> Result<()> {
        // Implementation would analyze incoming metrics for immediate issues
        Ok(())
    }

    /// Check for quality-related alerts
    fn check_quality_alerts(&self) -> Result<()> {
        // Implementation would check quality metrics against thresholds
        Ok(())
    }

    /// Analyze error patterns for insights
    fn analyze_error_patterns(&self) -> Result<()> {
        // Implementation would analyze error trends and patterns
        Ok(())
    }

    /// Setup default health checks
    fn setup_default_health_checks(&self) -> Result<()> {
        let mut health_checker = self.health_checker.lock().map_err(|e| {
            ShaclAiError::ShapeManagement(format!("Failed to lock health checker: {e}"))
        })?;

        // Add default health checks
        health_checker.add_health_check(HealthCheck {
            name: "memory_usage".to_string(),
            component: "system".to_string(),
            check_type: HealthCheckType::Memory,
            interval_secs: 60,
            timeout_secs: 10,
            enabled: true,
            critical: true,
        });

        health_checker.add_health_check(HealthCheck {
            name: "cpu_usage".to_string(),
            component: "system".to_string(),
            check_type: HealthCheckType::CPU,
            interval_secs: 30,
            timeout_secs: 5,
            enabled: true,
            critical: true,
        });

        Ok(())
    }

    /// Setup default alert rules
    fn setup_default_alert_rules(&self) -> Result<()> {
        let mut alert_manager = self.alert_manager.lock().map_err(|e| {
            ShaclAiError::ShapeManagement(format!("Failed to lock alert manager: {e}"))
        })?;

        // Add default alert rules
        alert_manager.add_alert_rule(AlertRule {
            id: "high_response_time".to_string(),
            name: "High Response Time".to_string(),
            condition: AlertCondition::GreaterThan,
            threshold: self.config.alert_thresholds.max_response_time_ms,
            duration_secs: 300,
            severity: AlertSeverity::Warning,
            enabled: true,
            notification_channels: self.config.notification_channels.clone(),
            auto_resolve: true,
            escalation_rules: Vec::new(),
        })?;

        alert_manager.add_alert_rule(AlertRule {
            id: "high_error_rate".to_string(),
            name: "High Error Rate".to_string(),
            condition: AlertCondition::GreaterThan,
            threshold: self.config.alert_thresholds.max_error_rate,
            duration_secs: 180,
            severity: AlertSeverity::Critical,
            enabled: true,
            notification_channels: self.config.notification_channels.clone(),
            auto_resolve: true,
            escalation_rules: Vec::new(),
        })?;

        Ok(())
    }

    /// Setup notification channels
    fn setup_notification_channels(&self) -> Result<()> {
        // Implementation would setup configured notification channels
        Ok(())
    }
}

/// Monitoring statistics
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct MonitoringStatistics {
    pub metrics_collected: u64,
    pub alerts_triggered: u32,
    pub uptime_seconds: u64,
    pub data_points_stored: u64,
    pub health_checks_performed: u32,
    pub notifications_sent: u32,
}

// Implementation stubs for various components
impl MetricsCollector {
    fn new() -> Self {
        Self {
            performance_metrics: VecDeque::new(),
            quality_metrics: VecDeque::new(),
            error_metrics: VecDeque::new(),
            system_metrics: VecDeque::new(),
            custom_metrics: HashMap::new(),
            collection_start_time: Instant::now(),
            last_collection_time: None,
        }
    }

    fn add_performance_metric(&mut self, metric: PerformanceMetric) -> Result<()> {
        self.performance_metrics.push_back(metric);
        self.last_collection_time = Some(Instant::now());
        Ok(())
    }

    fn add_quality_metric(&mut self, metric: QualityMetric) -> Result<()> {
        self.quality_metrics.push_back(metric);
        Ok(())
    }

    fn add_error_metric(&mut self, metric: ErrorMetric) -> Result<()> {
        self.error_metrics.push_back(metric);
        Ok(())
    }

    fn add_custom_metric(&mut self, name: String, metric: CustomMetric) -> Result<()> {
        self.custom_metrics
            .entry(name)
            .or_default()
            .push_back(metric);
        Ok(())
    }

    fn total_metrics_count(&self) -> u64 {
        (self.performance_metrics.len()
            + self.quality_metrics.len()
            + self.error_metrics.len()
            + self.system_metrics.len()) as u64
    }

    fn uptime_seconds(&self) -> u64 {
        self.collection_start_time.elapsed().as_secs()
    }

    fn data_points_count(&self) -> u64 {
        self.total_metrics_count()
    }
}

impl AlertManager {
    fn new() -> Self {
        Self {
            active_alerts: HashMap::new(),
            alert_history: VecDeque::new(),
            alert_rules: Vec::new(),
            notification_queue: VecDeque::new(),
            suppression_rules: Vec::new(),
        }
    }

    fn get_active_alerts(&self) -> Vec<Alert> {
        self.active_alerts.values().cloned().collect()
    }

    fn add_alert_rule(&mut self, rule: AlertRule) -> Result<()> {
        self.alert_rules.push(rule);
        Ok(())
    }

    fn total_alerts_count(&self) -> u32 {
        self.alert_history.len() as u32
    }
}

impl MonitoringDashboard {
    fn new() -> Self {
        Self {
            last_updated: Utc::now(),
            overall_health: SystemHealth::Healthy,
            performance_summary: PerformanceSummary {
                avg_response_time_ms: 100.0,
                current_throughput: 1000.0,
                peak_throughput_24h: 1500.0,
                error_rate_percent: 0.1,
                availability_percent: 99.9,
                response_time_trend: TrendDirection::Stable,
                throughput_trend: TrendDirection::Improving,
                performance_score: 0.95,
            },
            quality_summary: QualitySummary {
                overall_quality_score: 0.92,
                quality_trend: QualityTrend::Stable,
                issues_detected: 5,
                shapes_validated_24h: 10000,
                quality_improvement_percent: 2.5,
                data_completeness_percent: 98.5,
                consistency_score: 0.94,
            },
            error_summary: ErrorSummary {
                total_errors_24h: 12,
                critical_errors_24h: 1,
                error_rate_trend: TrendDirection::Improving,
                mean_time_to_resolution_min: 15.5,
                most_frequent_error_type: ErrorType::ValidationError,
                resolved_errors_24h: 11,
                unresolved_errors: 1,
            },
            active_alerts: Vec::new(),
            trends: TrendAnalysis {
                performance_trends: HashMap::new(),
                quality_trends: HashMap::new(),
                error_trends: HashMap::new(),
                usage_trends: HashMap::new(),
                seasonal_patterns: Vec::new(),
                anomalies_detected: Vec::new(),
            },
            recommendations: Vec::new(),
            uptime_stats: UptimeStatistics {
                current_uptime_secs: 86400,
                uptime_percent_24h: 99.95,
                uptime_percent_7d: 99.8,
                uptime_percent_30d: 99.9,
                longest_uptime_secs: 259200,
                total_downtime_24h_secs: 43,
                outage_count_24h: 1,
                mean_time_between_failures_hours: 168.0,
            },
            resource_utilization: ResourceUtilization {
                cpu_usage_percent: 45.2,
                memory_usage_percent: 67.8,
                disk_usage_percent: 23.1,
                network_usage_mb_s: 125.4,
                connection_count: 256,
                thread_count: 48,
                file_descriptor_count: 1024,
                resource_trends: HashMap::new(),
            },
        }
    }
}

impl HealthChecker {
    fn new() -> Self {
        Self {
            component_status: HashMap::new(),
            health_checks: Vec::new(),
            last_check_time: None,
            check_history: VecDeque::new(),
        }
    }

    fn add_health_check(&mut self, check: HealthCheck) {
        self.health_checks.push(check);
    }

    fn run_all_checks(&mut self) -> Result<Vec<HealthCheckResult>> {
        let mut results = Vec::new();
        let now = Utc::now();

        for check in &self.health_checks {
            if !check.enabled {
                continue;
            }

            let result = self.run_single_check(check, now)?;
            results.push(result);
        }

        self.last_check_time = Some(Instant::now());
        Ok(results)
    }

    fn run_single_check(
        &self,
        check: &HealthCheck,
        timestamp: DateTime<Utc>,
    ) -> Result<HealthCheckResult> {
        // Simplified health check implementation
        let status = match check.check_type {
            HealthCheckType::Memory => {
                // Simulate memory check
                if ({ let mut random = Random::default(); random.gen::<f64>() }) > 0.1 {
                    HealthStatus::Healthy
                } else {
                    HealthStatus::Warning
                }
            }
            HealthCheckType::CPU => {
                // Simulate CPU check
                if ({ let mut random = Random::default(); random.gen::<f64>() }) > 0.05 {
                    HealthStatus::Healthy
                } else {
                    HealthStatus::Critical
                }
            }
            _ => HealthStatus::Healthy,
        };

        Ok(HealthCheckResult {
            check_name: check.name.clone(),
            component: check.component.clone(),
            timestamp,
            status,
            response_time_ms: ({ let mut random = Random::default(); random.gen::<f64>() }) * 100.0,
            message: "Health check completed".to_string(),
            error: None,
        })
    }

    fn get_overall_health(&self) -> SystemHealth {
        // Simplified overall health calculation
        if self.component_status.values().any(|h| h.health_score < 0.5) {
            SystemHealth::Critical
        } else if self.component_status.values().any(|h| h.health_score < 0.8) {
            SystemHealth::Warning
        } else {
            SystemHealth::Healthy
        }
    }
}

impl MonitoringStorage {
    fn new() -> Self {
        Self {
            metrics_buffer: HashMap::new(),
            aggregated_data: HashMap::new(),
            retention_policy: RetentionPolicy {
                raw_data_days: 7,
                hourly_aggregation_days: 30,
                daily_aggregation_days: 90,
                monthly_aggregation_days: 365,
                compression_threshold_days: 30,
            },
            compression_enabled: true,
            backup_enabled: true,
        }
    }
}

impl NotificationEngine {
    fn new() -> Self {
        Self {
            channels: HashMap::new(),
            templates: HashMap::new(),
            delivery_status: HashMap::new(),
            rate_limits: HashMap::new(),
        }
    }
}

impl Default for SystemMonitor {
    fn default() -> Self {
        Self::new()
    }
}

/// Advanced anomaly detection engine for intelligent monitoring
#[derive(Debug)]
pub struct AnomalyDetector {
    detection_models: HashMap<String, AnomalyModel>,
    baseline_profiles: HashMap<String, BaselineProfile>,
    detection_config: AnomalyDetectionConfig,
    anomaly_history: VecDeque<AnomalyEvent>,
}

/// Configuration for anomaly detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyDetectionConfig {
    /// Sensitivity threshold (0.0 = very sensitive, 1.0 = very conservative)
    pub sensitivity_threshold: f64,

    /// Learning period in hours for establishing baselines
    pub learning_period_hours: u64,

    /// Enable statistical anomaly detection
    pub enable_statistical_detection: bool,

    /// Enable machine learning anomaly detection
    pub enable_ml_detection: bool,

    /// Enable pattern-based anomaly detection
    pub enable_pattern_detection: bool,

    /// Minimum confidence for anomaly alerts
    pub min_anomaly_confidence: f64,

    /// Window size for rolling statistics
    pub rolling_window_size: usize,

    /// Enable seasonal decomposition
    pub enable_seasonal_decomposition: bool,
}

impl Default for AnomalyDetectionConfig {
    fn default() -> Self {
        Self {
            sensitivity_threshold: 0.7,
            learning_period_hours: 24,
            enable_statistical_detection: true,
            enable_ml_detection: true,
            enable_pattern_detection: true,
            min_anomaly_confidence: 0.8,
            rolling_window_size: 50,
            enable_seasonal_decomposition: true,
        }
    }
}

/// Anomaly detection model
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyModel {
    pub model_type: AnomalyModelType,
    pub training_data_size: usize,
    pub last_training: DateTime<Utc>,
    pub accuracy_score: f64,
    pub parameters: HashMap<String, f64>,
    pub feature_importance: HashMap<String, f64>,
}

/// Types of anomaly detection models
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AnomalyModelType {
    /// Statistical methods (Z-score, IQR, etc.)
    Statistical,
    /// Isolation Forest
    IsolationForest,
    /// One-Class SVM
    OneClassSVM,
    /// Local Outlier Factor
    LocalOutlierFactor,
    /// Autoencoder neural networks
    Autoencoder,
    /// Temporal pattern analysis
    TemporalPattern,
}

/// Baseline performance profile for normal operation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BaselineProfile {
    pub metric_name: String,
    pub mean: f64,
    pub std_dev: f64,
    pub median: f64,
    pub percentile_95: f64,
    pub percentile_99: f64,
    pub min_value: f64,
    pub max_value: f64,
    pub sample_count: usize,
    pub last_updated: DateTime<Utc>,
    pub seasonal_patterns: Option<SeasonalPatternData>,
}

/// Seasonal pattern information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SeasonalPatternData {
    pub daily_patterns: Vec<f64>,
    pub weekly_patterns: Vec<f64>,
    pub monthly_patterns: Option<Vec<f64>>,
    pub trend_component: f64,
    pub seasonal_strength: f64,
}

/// Anomaly event detected by the system
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnomalyEvent {
    pub event_id: String,
    pub timestamp: DateTime<Utc>,
    pub metric_name: String,
    pub observed_value: f64,
    pub expected_value: f64,
    pub anomaly_score: f64,
    pub confidence: f64,
    pub anomaly_type: AnomalyType,
    pub detection_method: AnomalyModelType,
    pub impact_assessment: ImpactAssessment,
    pub related_metrics: Vec<String>,
    pub root_cause_hints: Vec<String>,
}

/// Types of anomalies
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AnomalyType {
    /// Value significantly higher than expected
    PositiveSpike,
    /// Value significantly lower than expected
    NegativeDip,
    /// Sudden change in trend
    TrendChange,
    /// Pattern disruption
    PatternDisruption,
    /// Correlation breakdown
    CorrelationBreakdown,
    /// Seasonal deviation
    SeasonalDeviation,
}

/// Impact assessment for anomalies
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ImpactAssessment {
    pub severity: AnomalySeverity,
    pub affected_components: Vec<String>,
    pub business_impact: BusinessImpact,
    pub estimated_user_impact: f64,
    pub recovery_time_estimate: Option<Duration>,
}

/// Anomaly severity levels
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub enum AnomalySeverity {
    Low,
    Medium,
    High,
    Critical,
}

/// Business impact categories
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum BusinessImpact {
    Negligible,
    Minor,
    Moderate,
    Significant,
    Severe,
}

impl AnomalyDetector {
    /// Create a new anomaly detector
    pub fn new() -> Self {
        Self::with_config(AnomalyDetectionConfig::default())
    }

    /// Create with custom configuration
    pub fn with_config(config: AnomalyDetectionConfig) -> Self {
        Self {
            detection_models: HashMap::new(),
            baseline_profiles: HashMap::new(),
            detection_config: config,
            anomaly_history: VecDeque::new(),
        }
    }

    /// Analyze metrics for anomalies
    pub fn analyze_metrics(&mut self, metrics: &[PerformanceMetric]) -> Result<Vec<AnomalyEvent>> {
        let mut anomalies = Vec::new();

        for metric in metrics {
            // Statistical anomaly detection
            if self.detection_config.enable_statistical_detection {
                if let Some(stat_anomaly) = self.detect_statistical_anomaly(metric)? {
                    anomalies.push(stat_anomaly);
                }
            }

            // Pattern-based anomaly detection
            if self.detection_config.enable_pattern_detection {
                if let Some(pattern_anomaly) = self.detect_pattern_anomaly(metric)? {
                    anomalies.push(pattern_anomaly);
                }
            }

            // ML-based anomaly detection
            if self.detection_config.enable_ml_detection {
                if let Some(ml_anomaly) = self.detect_ml_anomaly(metric)? {
                    anomalies.push(ml_anomaly);
                }
            }
        }

        // Store anomalies in history
        for anomaly in &anomalies {
            self.anomaly_history.push_back(anomaly.clone());

            // Limit history size
            if self.anomaly_history.len() > 1000 {
                self.anomaly_history.pop_front();
            }
        }

        Ok(anomalies)
    }

    /// Detect statistical anomalies using z-score and IQR methods
    fn detect_statistical_anomaly(
        &mut self,
        metric: &PerformanceMetric,
    ) -> Result<Option<AnomalyEvent>> {
        let metric_name = "response_time_ms";
        let value = metric.response_time_ms;

        // Get or create baseline profile
        if !self.baseline_profiles.contains_key(metric_name) {
            self.initialize_baseline_profile(metric_name, value)?;
            return Ok(None); // Not enough data yet
        }

        let baseline = self.baseline_profiles.get(metric_name).unwrap();

        // Z-score anomaly detection
        let z_score = (value - baseline.mean) / baseline.std_dev;
        let z_threshold = 3.0 * (1.0 - self.detection_config.sensitivity_threshold); // More sensitive = lower threshold

        if z_score.abs() > z_threshold {
            let anomaly_score = z_score.abs() / z_threshold;
            let confidence = (anomaly_score - 1.0).clamp(0.0, 1.0);

            if confidence >= self.detection_config.min_anomaly_confidence {
                let anomaly_type = if z_score > 0.0 {
                    AnomalyType::PositiveSpike
                } else {
                    AnomalyType::NegativeDip
                };

                // Generate root cause hints before moving anomaly_type
                let root_cause_hints = self.generate_root_cause_hints(metric_name, &anomaly_type);

                let event = AnomalyEvent {
                    event_id: Uuid::new_v4().to_string(),
                    timestamp: metric.timestamp,
                    metric_name: metric_name.to_string(),
                    observed_value: value,
                    expected_value: baseline.mean,
                    anomaly_score,
                    confidence,
                    anomaly_type,
                    detection_method: AnomalyModelType::Statistical,
                    impact_assessment: self.assess_impact(metric_name, anomaly_score),
                    related_metrics: vec![
                        "cpu_usage_percent".to_string(),
                        "memory_usage_mb".to_string(),
                    ],
                    root_cause_hints,
                };

                return Ok(Some(event));
            }
        }

        Ok(None)
    }

    /// Detect pattern-based anomalies
    fn detect_pattern_anomaly(&self, metric: &PerformanceMetric) -> Result<Option<AnomalyEvent>> {
        // Check for correlation breakdown between related metrics
        let response_time = metric.response_time_ms;
        let cpu_usage = metric.cpu_usage_percent;

        // Normally, high CPU should correlate with higher response times
        // If response time is high but CPU is low, that's anomalous
        if response_time > 1000.0 && cpu_usage < 20.0 {
            let anomaly_score = response_time / 1000.0 * (1.0 - cpu_usage / 100.0);
            let confidence = 0.8; // Pattern-based confidence

            if confidence >= self.detection_config.min_anomaly_confidence {
                let event = AnomalyEvent {
                    event_id: Uuid::new_v4().to_string(),
                    timestamp: metric.timestamp,
                    metric_name: "response_time_cpu_correlation".to_string(),
                    observed_value: response_time,
                    expected_value: cpu_usage * 10.0, // Simplified expected correlation
                    anomaly_score,
                    confidence,
                    anomaly_type: AnomalyType::CorrelationBreakdown,
                    detection_method: AnomalyModelType::TemporalPattern,
                    impact_assessment: self.assess_impact("correlation", anomaly_score),
                    related_metrics: vec![
                        "response_time_ms".to_string(),
                        "cpu_usage_percent".to_string(),
                    ],
                    root_cause_hints: vec![
                        "I/O bottleneck possible".to_string(),
                        "Database connection issues".to_string(),
                        "Network latency".to_string(),
                    ],
                };

                return Ok(Some(event));
            }
        }

        Ok(None)
    }

    /// Detect ML-based anomalies (simplified implementation)
    fn detect_ml_anomaly(&self, _metric: &PerformanceMetric) -> Result<Option<AnomalyEvent>> {
        // Placeholder for ML-based anomaly detection
        // In a full implementation, this would use trained ML models
        // like Isolation Forest, One-Class SVM, or Autoencoders
        Ok(None)
    }

    /// Initialize baseline profile for a metric
    fn initialize_baseline_profile(&mut self, metric_name: &str, initial_value: f64) -> Result<()> {
        let profile = BaselineProfile {
            metric_name: metric_name.to_string(),
            mean: initial_value,
            std_dev: 0.0,
            median: initial_value,
            percentile_95: initial_value,
            percentile_99: initial_value,
            min_value: initial_value,
            max_value: initial_value,
            sample_count: 1,
            last_updated: Utc::now(),
            seasonal_patterns: None,
        };

        self.baseline_profiles
            .insert(metric_name.to_string(), profile);
        Ok(())
    }

    /// Assess the impact of an anomaly
    fn assess_impact(&self, metric_name: &str, anomaly_score: f64) -> ImpactAssessment {
        let severity = if anomaly_score > 5.0 {
            AnomalySeverity::Critical
        } else if anomaly_score > 3.0 {
            AnomalySeverity::High
        } else if anomaly_score > 2.0 {
            AnomalySeverity::Medium
        } else {
            AnomalySeverity::Low
        };

        let business_impact = match severity {
            AnomalySeverity::Critical => BusinessImpact::Severe,
            AnomalySeverity::High => BusinessImpact::Significant,
            AnomalySeverity::Medium => BusinessImpact::Moderate,
            AnomalySeverity::Low => BusinessImpact::Minor,
        };

        let affected_components = match metric_name {
            "response_time_ms" => vec!["web_server".to_string(), "database".to_string()],
            "cpu_usage_percent" => vec!["compute_engine".to_string()],
            "memory_usage_mb" => vec!["memory_manager".to_string()],
            _ => vec!["unknown".to_string()],
        };

        ImpactAssessment {
            severity,
            affected_components,
            business_impact,
            estimated_user_impact: anomaly_score / 10.0, // Simplified calculation
            recovery_time_estimate: Some(Duration::from_secs(300)), // 5 minutes estimate
        }
    }

    /// Generate root cause hints for anomalies
    fn generate_root_cause_hints(
        &self,
        metric_name: &str,
        anomaly_type: &AnomalyType,
    ) -> Vec<String> {
        match (metric_name, anomaly_type) {
            ("response_time_ms", AnomalyType::PositiveSpike) => vec![
                "High database load".to_string(),
                "Network congestion".to_string(),
                "Resource contention".to_string(),
                "Cache miss spike".to_string(),
            ],
            ("cpu_usage_percent", AnomalyType::PositiveSpike) => vec![
                "CPU-intensive operation".to_string(),
                "Infinite loop or deadlock".to_string(),
                "Background process consuming CPU".to_string(),
            ],
            ("memory_usage_mb", AnomalyType::PositiveSpike) => vec![
                "Memory leak detected".to_string(),
                "Large dataset processing".to_string(),
                "Caching strategy ineffective".to_string(),
            ],
            _ => vec!["Unknown root cause".to_string()],
        }
    }

    /// Get anomaly history
    pub fn get_anomaly_history(&self) -> &VecDeque<AnomalyEvent> {
        &self.anomaly_history
    }

    /// Update baseline profiles with new data
    pub fn update_baselines(&mut self, metrics: &[PerformanceMetric]) -> Result<()> {
        for metric in metrics {
            self.update_baseline_for_metric("response_time_ms", metric.response_time_ms)?;
            self.update_baseline_for_metric("cpu_usage_percent", metric.cpu_usage_percent)?;
            self.update_baseline_for_metric("memory_usage_mb", metric.memory_usage_mb)?;
        }
        Ok(())
    }

    /// Update baseline profile for a specific metric
    fn update_baseline_for_metric(&mut self, metric_name: &str, value: f64) -> Result<()> {
        if let Some(profile) = self.baseline_profiles.get_mut(metric_name) {
            // Update running statistics
            let n = profile.sample_count as f64;
            let new_mean = (profile.mean * n + value) / (n + 1.0);
            let new_variance =
                ((n - 1.0) * profile.std_dev.powi(2) + (value - new_mean).powi(2)) / n;

            profile.mean = new_mean;
            profile.std_dev = new_variance.sqrt();
            profile.min_value = profile.min_value.min(value);
            profile.max_value = profile.max_value.max(value);
            profile.sample_count += 1;
            profile.last_updated = Utc::now();
        }
        Ok(())
    }
}

impl Default for AnomalyDetector {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_system_monitor_creation() {
        let monitor = SystemMonitor::new();
        assert!(monitor.config.enable_real_time);
        assert_eq!(monitor.config.collection_interval_secs, 60);
    }

    #[test]
    fn test_monitoring_config_default() {
        let config = MonitoringConfig::default();
        assert!(config.enable_alerting);
        assert!(config.enable_performance_tracking);
        assert!(config.enable_quality_tracking);
        assert!(config.enable_error_tracking);
    }

    #[test]
    fn test_alert_thresholds_default() {
        let thresholds = AlertThresholds::default();
        assert_eq!(thresholds.max_response_time_ms, 5000.0);
        assert_eq!(thresholds.min_quality_score, 0.8);
        assert_eq!(thresholds.max_error_rate, 0.05);
    }

    #[test]
    fn test_health_status_ordering() {
        assert!(HealthStatus::Critical > HealthStatus::Warning);
        assert!(HealthStatus::Warning > HealthStatus::Healthy);
    }

    #[test]
    fn test_metrics_collector() {
        let mut collector = MetricsCollector::new();

        let metric = PerformanceMetric {
            timestamp: Utc::now(),
            response_time_ms: 150.0,
            throughput: 1000.0,
            concurrent_requests: 10,
            memory_usage_mb: 512.0,
            cpu_usage_percent: 45.0,
            disk_io_mb_s: 10.0,
            network_io_mb_s: 25.0,
            gc_time_ms: Some(5.0),
            cache_hit_rate: Some(0.85),
            tags: HashMap::new(),
        };

        collector.add_performance_metric(metric).unwrap();
        assert_eq!(collector.performance_metrics.len(), 1);
        assert!(collector.total_metrics_count() > 0);
    }

    #[test]
    fn test_dashboard_creation() {
        let dashboard = MonitoringDashboard::new();
        assert_eq!(dashboard.overall_health, SystemHealth::Healthy);
        assert!(dashboard.performance_summary.performance_score > 0.0);
        assert!(dashboard.quality_summary.overall_quality_score > 0.0);
    }
}
